{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5811f0a7",
   "metadata": {},
   "source": [
    "# Training tree-based models for time series forecasting\n",
    "\n",
    "After working with the ARIMA model yesterday, today you will use the same data to fit a Random Forest (RF) and XGBoost model on it.\n",
    "\n",
    "Throughout the seminar, we will use the following splits for training, validation, and testing. Make sure to keep the tests unseen until the final evaluation (information leakage):\n",
    "\n",
    "- Training set: 2009-2013\n",
    "- Validation set: 2014\n",
    "- Test set: 2015-2016\n",
    "\n",
    "Required python packages: pandas, numpy, matplotlib, scikit-learn, xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a00f709",
   "metadata": {},
   "source": [
    "## Task 1: Feature Engineering\n",
    "\n",
    "For random forest and xgboost, engineer features for the flu-trends dataset by incorporating lagged values (e.g., 8 weeks); ensure that your feature engineering process does not introduce information from future data points into past records.\n",
    "\n",
    "> For instance, use the past 8 weeks to predict the next week.X-columns: FluVisits_t-8, FluVisits_t-7, ..., FluVisits_t-1\n",
    ">\n",
    "> You can also predict more than one week ahead. Then you need to shift the next samples accordingly.X-columns: FluVisits_t-8, FluVisits_t-7, ..., FluVisits_t-1\n",
    "Y-columns: FluVisits_t, FluVisits_t+1, FluVisits_t+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f3232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "110a363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join('..', 'data', 'interim', 'test.csv'))\n",
    "train = pd.read_csv(os.path.join('..', 'data', 'interim', 'training.csv'))\n",
    "val = pd.read_csv(os.path.join('..', 'data', 'interim', 'validation.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb45686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 132)\n",
      "(236, 132)\n",
      "(52, 132)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "print(train.shape)\n",
    "print(val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe0b60",
   "metadata": {},
   "source": [
    "```python\n",
    " def create_features(data: pd.DataFrame, target_column: str, feature_length: int = 8, prediction_length: int = 2) -> [np.ndarray, np.ndarray]:\n",
    "    ...\n",
    "    return X, y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82f43ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data: pd.DataFrame, target_column: str, feature_length: int = 8, prediction_length: int = 2) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create features and target arrays for time series forecasting.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data containing the time series.\n",
    "        target_column (str): The name of the target column to predict.\n",
    "        feature_length (int, optional): The number of past observations to use as features. Defaults to 8.\n",
    "        prediction_length (int, optional): The number of future observations to predict. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: The feature and target arrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    target_data = data[target_column].values\n",
    "    for i in range(feature_length, len(data) - prediction_length + 1): # Iterate over rows from e.g. 8 to len(data)-2\n",
    "        features = target_data[i-feature_length:i] # Get the last 8 values as features as np array\n",
    "        target = target_data[i:i+prediction_length] # Get the next 2 values as target as np array\n",
    "        # for first iteration, initialize X and y\n",
    "        if i == feature_length:\n",
    "            X = np.array([features])\n",
    "            y = np.array(target)\n",
    "        else: # Add new rows to ndarrays\n",
    "            X = np.vstack([X, features])\n",
    "            y = np.vstack([y, target])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee68a709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((227, 8), (227, 2))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = create_features(train, target_column=\"FluVisits\", prediction_length=2, feature_length=8)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0a544",
   "metadata": {},
   "source": [
    "## Task 2: Fitting the models\n",
    "\n",
    "Task 2.1: Fit a Random Forest model to the data. Evaluate the model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124bf1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aff02de",
   "metadata": {},
   "source": [
    "Task 2.2: Fit a XGBoost model to the data. Evaluate the model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b7487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "596b82d8",
   "metadata": {},
   "source": [
    "Describe the performance of the models and compare them in a few sentences. How do they perform in comparison to each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae78a1",
   "metadata": {},
   "source": [
    "## Task 3: Hyperparameter tuning via grid search\n",
    "\n",
    "Tune the hyperparameters of the Random Forest and XGBoost models using grid search. Focus on the following hyperparameters:\n",
    "\n",
    "Random Forest:\n",
    "- n_estimators\n",
    "- max_depth\n",
    "- ...\n",
    "\n",
    "XGBoost:\n",
    "- gamma\n",
    "- max_depth\n",
    "- eta\n",
    "- ...\n",
    "\n",
    "Optimize the hyperparameters using the validation set and the refit the models using the best hyperparameters on the entire training set and evaluate them on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b256f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b26dfa0",
   "metadata": {},
   "source": [
    "## Task 4: Time series cross-validation\n",
    "\n",
    "Implement a time series cross-validation strategy to optimize the hyperparameters of the models. Except for the cross-validation strategy, the procedure is the same as in Task 3. You only need to define different training and validation sets before performing the grid search. In the end, you take the parameters that performed best across all folds and refit the models on the entire training set and evaluate them on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952497e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd6b19dc",
   "metadata": {},
   "source": [
    "# Optional Task:\n",
    "- Take a look which other features could be good predictors for the flu visits. How does including additional features impact the overall model performance?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
