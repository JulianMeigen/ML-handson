{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulianMeigen/ML-handson/blob/main/notebooks/7.0-SNJMMH-Day7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7e28cad",
      "metadata": {
        "id": "d7e28cad"
      },
      "source": [
        "# Assignment Day 7\n",
        "\n",
        "## Team members:\n",
        "- Samuel Nebgen s6sanebg@uni-bonn.de\n",
        "- Muhammad Humza Arain s27marai@uni-bonn.de\n",
        "- Julian Meigen s82jmeig@uni-bonn.de\n",
        "\n",
        "## 16.09.2025\n",
        "\n",
        "Contributions were made by all team members in around the same amount, either based on discussions or coding."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/1VESm-JaHEqPJmM23iLW1mEJsuI2mLBdx?usp=sharing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMu2EYgDpjh1",
        "outputId": "7e67668e-53df-4459-cded-4dde4f990af9"
      },
      "id": "LMu2EYgDpjh1",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1i6W9fI3sGEn6V9xBlt2MxlonZXOTLZjg load-subgraph_doc.ipynb\n",
            "Processing file 1qZpQzFMRzuYQ0xoQJcUNe7CRBMS2mRmz subgraph_hop_1.pt\n",
            "Processing file 1iz_FOBs9k7m9z3lDtRIXzRkd92tL_EK- subgraph.pt\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1i6W9fI3sGEn6V9xBlt2MxlonZXOTLZjg\n",
            "To: /content/ML-HandsOn/load-subgraph_doc.ipynb\n",
            "100% 2.92k/2.92k [00:00<00:00, 7.56MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qZpQzFMRzuYQ0xoQJcUNe7CRBMS2mRmz\n",
            "To: /content/ML-HandsOn/subgraph_hop_1.pt\n",
            "100% 10.5M/10.5M [00:00<00:00, 29.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1iz_FOBs9k7m9z3lDtRIXzRkd92tL_EK-\n",
            "From (redirected): https://drive.google.com/uc?id=1iz_FOBs9k7m9z3lDtRIXzRkd92tL_EK-&confirm=t&uuid=2d20b6f1-903c-4855-9bd7-0a9a044e9775\n",
            "To: /content/ML-HandsOn/subgraph.pt\n",
            "100% 1.64G/1.64G [00:18<00:00, 88.7MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "Ivyr61KCaemq"
      },
      "id": "Ivyr61KCaemq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import plotly\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch.nn import Embedding\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv"
      ],
      "metadata": {
        "id": "udctacDr0tZ3"
      },
      "id": "udctacDr0tZ3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9d575080",
      "metadata": {
        "id": "9d575080"
      },
      "source": [
        "# Task 1 Perform a node labeling task with a Graph ML model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6869181a",
      "metadata": {
        "id": "6869181a"
      },
      "source": [
        "## a) Load the graph dataset (ogbn-proteins) into pytorch-geometric\n",
        "\n",
        "We are directly using a Subgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "\n",
        "path_big = \"/content/ML-HandsOn/subgraph.pt\"\n",
        "path_small = \"/content/ML-HandsOn/subgraph_hop_1.pt\"\n",
        "\n",
        "dataset = torch.load(path_small, weights_only=False)\n",
        "\n",
        "data = dataset[\"graph\"]\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "AMYvOlq0Oai9"
      },
      "id": "AMYvOlq0Oai9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = to_networkx(data, to_undirected=True)\n",
        "print(G)"
      ],
      "metadata": {
        "id": "PLGZSjgMQLwW"
      },
      "id": "PLGZSjgMQLwW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "adb891a2",
      "metadata": {
        "id": "adb891a2"
      },
      "source": [
        "## b) Create a train, val, test split on the nodes or load the masks via pytorch-geometric."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2e4f3ec",
      "metadata": {
        "id": "c2e4f3ec"
      },
      "source": [
        "### i. Create a subgraph if the computation is too expensive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_nodes = data.num_nodes\n",
        "perm = torch.randperm(num_nodes)\n",
        "\n",
        "train_size = int(0.7 * num_nodes)\n",
        "val_size = int(0.15 * num_nodes)\n",
        "\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "train_mask[perm[:train_size]] = True\n",
        "val_mask[perm[train_size:train_size + val_size]] = True\n",
        "test_mask[perm[train_size + val_size:]] = True\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "print(data.y)"
      ],
      "metadata": {
        "id": "xOKEF5ISN1y2"
      },
      "id": "xOKEF5ISN1y2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data.y[data.train_mask]))\n",
        "print(len(data.y[data.val_mask]))\n",
        "print(len(data.y[data.test_mask]))"
      ],
      "metadata": {
        "id": "_4mXk0BvSgBk"
      },
      "id": "_4mXk0BvSgBk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "be9b9d6f",
      "metadata": {
        "id": "be9b9d6f"
      },
      "source": [
        "## c) Initialize the graph with random node embeddings."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwuG7Sljr1Nb"
      },
      "id": "bwuG7Sljr1Nb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_nodes = data.num_nodes\n",
        "embedding_dim = 64\n",
        "x = torch.empty((num_nodes, embedding_dim))  # empty tensor\n",
        "torch.nn.init.xavier_uniform_(x)  # Xavier uniform initialization\n",
        "data.x = x"
      ],
      "metadata": {
        "id": "3i_8zJXRT_P0"
      },
      "id": "3i_8zJXRT_P0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_idx = data.edge_index.flatten().unique()\n",
        "train_idx = torch.tensor(node_idx[data.train_mask.numpy()])\n",
        "test_idx = torch.tensor(node_idx[data.test_mask.numpy()])\n",
        "val_idx = torch.tensor(node_idx[data.val_mask.numpy()])\n",
        "\n",
        "train_subgraph = data.subgraph(train_idx)\n",
        "test_subgraph = data.subgraph(test_idx)\n",
        "val_subgraph = data.subgraph(val_idx)"
      ],
      "metadata": {
        "id": "vXrSAbHZjVpW"
      },
      "id": "vXrSAbHZjVpW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8ed30d82",
      "metadata": {
        "id": "8ed30d82"
      },
      "source": [
        "## d) Define a graph convolutional neural network class with two layers using pytorch-geometric.."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_nodes, embedding_dim, hidden_dim, out_dim, drop_out=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(embedding_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
        "        self.dropout = torch.nn.Dropout(drop_out)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # apply GCN layers\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "X4YJgz5RV81h"
      },
      "id": "X4YJgz5RV81h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gcn = GCN(num_nodes=data.num_nodes, embedding_dim=64, hidden_dim=128, out_dim=112)"
      ],
      "metadata": {
        "id": "VCcuHNZ2Xfxr"
      },
      "id": "VCcuHNZ2Xfxr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "17ea0a7b",
      "metadata": {
        "id": "17ea0a7b"
      },
      "source": [
        "### i. Train your model on the train dataset using an optimizer and a loss function for a multilabel classification task for 100 epochs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer and loss\n",
        "optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "model_gcn.train()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    optimizer.zero_grad()\n",
        "    out = model_gcn(data.x, train_subgraph.edge_index)  # forward pass\n",
        "    loss = criterion(out, data.y.float())        # multi-label BCE loss\n",
        "    loss.backward()                       # backward pass\n",
        "    optimizer.step()                      # update parameters\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "d9QvaRtBiXbF"
      },
      "id": "d9QvaRtBiXbF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "db51457f",
      "metadata": {
        "id": "db51457f"
      },
      "source": [
        "### ii. Test your model on the test set and evaluate it with accuracy, AUROC, precision, recall and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "1C9Z1wzHq2WK"
      },
      "id": "1C9Z1wzHq2WK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gcn.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model_gcn(data.x, test_subgraph.edge_index)          # raw logits\n",
        "    probs = torch.sigmoid(logits)                   # convert to probabilities\n",
        "    preds = (probs > 0.5).int()                     # threshold at 0.5\n",
        "\n",
        "    y_true = data.y[test_mask].numpy()\n",
        "    y_pred = preds[test_mask].numpy()\n",
        "    y_prob = probs[test_mask].numpy()\n",
        "\n",
        "# Accuracy (exact match per node)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# AUROC (per class, average='macro')\n",
        "auroc = roc_auc_score(y_true, y_prob, average='macro')\n",
        "\n",
        "# Precision, Recall, F1 (micro-averaged)\n",
        "precision = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "\n",
        "print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Test AUROC:     {auroc:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall:    {recall:.4f}\")\n",
        "print(f\"Test F1 Score:  {f1:.4f}\")"
      ],
      "metadata": {
        "id": "5ALJsTVMk2_y"
      },
      "id": "5ALJsTVMk2_y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outer Cross-valiudation with Stratifiedkfold"
      ],
      "metadata": {
        "id": "IVdAkpJLtE8c"
      },
      "id": "IVdAkpJLtE8c"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "YAFIKasdqYsC"
      },
      "id": "YAFIKasdqYsC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t35mySw2t-UR"
      },
      "id": "t35mySw2t-UR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_idx = data.edge_index.flatten().unique()\n",
        "y = data.node_species.squeeze()\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "accuracys = []\n",
        "aurocs = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1s = []\n",
        "for train_idx, test_idx in skf.split(nodes_idx, y):\n",
        "  train_subgraph = data.subgraph(torch.tensor(train_idx))\n",
        "  test_subgraph = data.subgraph(torch.tensor(test_idx))\n",
        "\n",
        "  # Train the model\n",
        "  optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # Training loop\n",
        "  epochs = 100\n",
        "  model_gcn.train()\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      optimizer.zero_grad()\n",
        "      out = model_gcn(data.x, train_subgraph.edge_index)  # forward pass\n",
        "      loss = criterion(out, data.y.float())        # multi-label BCE loss\n",
        "      loss.backward()                       # backward pass\n",
        "      optimizer.step()                      # update parameters\n",
        "\n",
        "  # Evaluate the model\n",
        "  model_gcn.eval()\n",
        "  with torch.no_grad():\n",
        "      logits = model_gcn(data.x, test_subgraph.edge_index)          # raw logits\n",
        "      probs = torch.sigmoid(logits)                   # convert to probabilities\n",
        "      preds = (probs > 0.5).int()                     # threshold at 0.5\n",
        "\n",
        "      y_true = data.y[test_mask].numpy()\n",
        "      y_pred = preds[test_mask].numpy()\n",
        "      y_prob = probs[test_mask].numpy()\n",
        "\n",
        "      # Accuracy (exact match per node)\n",
        "      accuracy = accuracy_score(y_true, y_pred)\n",
        "      accuracys.append(accuracy)\n",
        "\n",
        "      # AUROC (per class, average='macro')\n",
        "      auroc = roc_auc_score(y_true, y_prob, average='macro')\n",
        "      aurocs.append(auroc)\n",
        "\n",
        "      # Precision, Recall, F1 (micro-averaged)\n",
        "      precision = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "      precisions.append(precision)\n",
        "      recall = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "      recalls.append(recall)\n",
        "      f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "      f1s.append(f1)\n",
        "      print(\"-----\")"
      ],
      "metadata": {
        "id": "VHvRf5VLtWmH"
      },
      "id": "VHvRf5VLtWmH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Test Accuracy:  {np.mean(accuracys):.4f}\")\n",
        "print(f\"Test AUROC:     {np.mean(aurocs):.4f}\")\n",
        "print(f\"Test Precision: {np.mean(precisions):.4f}\")\n",
        "print(f\"Test Recall:    {np.mean(recalls):.4f}\")\n",
        "print(f\"Test F1 Score:  {np.mean(f1s):.4f}\")"
      ],
      "metadata": {
        "id": "NuKqtFQhxHVY"
      },
      "id": "NuKqtFQhxHVY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "999eb937",
      "metadata": {
        "id": "999eb937"
      },
      "source": [
        "## e) Set up a hyperparameter optimization pipeline with nested 5-fold cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0330797",
      "metadata": {
        "id": "c0330797"
      },
      "source": [
        "### i. Familiarize yourself with the hyperparameter optimization package optuna (https://optuna.org/ )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna"
      ],
      "metadata": {
        "id": "C0NOJpmr28dR"
      },
      "id": "C0NOJpmr28dR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, train_subdata):\n",
        "  # Train the model\n",
        "  optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # Training loop\n",
        "  epochs = 100\n",
        "  model.train()\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      optimizer.zero_grad()\n",
        "      out = model(data.x, train_subgraph.edge_index)  # forward pass\n",
        "      loss = criterion(out, data.y.float())        # multi-label BCE loss\n",
        "      loss.backward()                       # backward pass\n",
        "      optimizer.step()                      # update parameters\n"
      ],
      "metadata": {
        "id": "wvKqXgZy5iS9"
      },
      "id": "wvKqXgZy5iS9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, data, val_subdata):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      logits = model(data.x, val_subdata.edge_index)          # raw logits\n",
        "      probs = torch.sigmoid(logits)                   # convert to probabilities\n",
        "      preds = (probs > 0.5).int()                     # threshold at 0.5\n",
        "\n",
        "      y_true = data.y[test_mask].numpy()\n",
        "      y_pred = preds[test_mask].numpy()\n",
        "      y_prob = probs[test_mask].numpy()\n",
        "\n",
        "      # Accuracy (exact match per node)\n",
        "      accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "      # AUROC (per class, average='macro')\n",
        "      auroc = roc_auc_score(y_true, y_prob, average='macro')\n",
        "\n",
        "      # Precision, Recall, F1 (micro-averaged)\n",
        "      precision = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "      recall = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "      f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "\n",
        "      wandb.log({\n",
        "        \"Test AUROC\": auroc,\n",
        "        \"Test Loss\": })\n"
      ],
      "metadata": {
        "id": "fPir5J4W8SLD"
      },
      "id": "fPir5J4W8SLD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNoptimization:\n",
        "    def __init__(self, data, train_subdata, val_subdata, study_name=\"GCN_optimization\"):\n",
        "        self.data = data\n",
        "        self.train_subdata = train_subdata\n",
        "        self.val_subdata = val_subdata\n",
        "\n",
        "        self.study_name = study_name\n",
        "        self.storage_name = \"sqlite:///{}.db\".format(self.study_name)\n",
        "        self.study = optuna.create_study(study_name=self.study_name, storage=self.storage_name, load_if_exists=True)\n",
        "\n",
        "    def objective(self, trial):\n",
        "        # Define the hyperparameters to optimize\n",
        "\n",
        "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.7)\n",
        "        hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 256)\n",
        "        embedding_dim = trial.suggest_int(\"embedding_dim\", 16, 256)\n",
        "\n",
        "        # Create the GCN model with the suggested hyperparameters\n",
        "        model = GCN(num_nodes=data.num_nodes, embedding_dim=embedding_dim, hidden_dim=hidden_dim, out_dim=112, drop_out=dropout)\n",
        "\n",
        "        # Train the model\n",
        "        optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training loop\n",
        "        epochs = 100\n",
        "        model.train()\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data.x, self.train_subgraph.edge_index)  # forward pass\n",
        "            loss = criterion(out, data.y.float())        # multi-label BCE loss\n",
        "            loss.backward()                       # backward pass\n",
        "            optimizer.step()                      # update parameters\n",
        "\n",
        "\n",
        "        # Validate the model\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(data.x, self.val_subdata.edge_index)          # raw logits\n",
        "            probs = torch.sigmoid(logits)                   # convert to probabilities\n",
        "            preds = (probs > 0.5).int()                     # threshold at 0.5\n",
        "\n",
        "            y_true = data.y[test_mask].numpy()\n",
        "            y_pred = preds[test_mask].numpy()\n",
        "            y_prob = probs[test_mask].numpy()\n",
        "\n",
        "            # Accuracy (exact match per node)\n",
        "            accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "            # AUROC (per class, average='macro')\n",
        "            auroc = roc_auc_score(y_true, y_prob, average='macro')\n",
        "\n",
        "            wandb.log({\n",
        "              \"Test AUROC\": auroc,\n",
        "              \"Test Loss\": })\n",
        "          return\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pvEDeDDl29qU"
      },
      "id": "pvEDeDDl29qU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "949b44b6",
      "metadata": {
        "id": "949b44b6"
      },
      "source": [
        "### ii. Integrate the logging package mlflow (https://mlflow.org/) to log your metrics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install optuna"
      ],
      "metadata": {
        "id": "NsG99E80ytLZ"
      },
      "id": "NsG99E80ytLZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wandb -q"
      ],
      "metadata": {
        "id": "US472ByzzOlT"
      },
      "id": "US472ByzzOlT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Ignore excessive warnings\n",
        "import logging\n",
        "logging.propagate = False\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "# WandB – Import the wandb library\n",
        "import wandb"
      ],
      "metadata": {
        "id": "gYKlEImA0tDG"
      },
      "id": "gYKlEImA0tDG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WandB – Login to your wandb account so you can log all your metrics\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "DKEoxPS505VQ"
      },
      "id": "DKEoxPS505VQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8GETLWDF1zFJ"
      },
      "id": "8GETLWDF1zFJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "06568c19",
      "metadata": {
        "id": "06568c19"
      },
      "source": [
        "### iii. Train and test your models and report the evaluation metrics with mean and std for the nested CV."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import optuna\n",
        "import wandb\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Assuming you already have your GCN model class defined as `GCN`\n",
        "\n",
        "class GCNOptimization:\n",
        "    def __init__(self, data, study_name=\"GCN_optimization\"):\n",
        "        self.data = data\n",
        "        self.study_name = study_name\n",
        "        self.storage_name = f\"sqlite:///{self.study_name}.db\"\n",
        "        self.study = optuna.create_study(\n",
        "            study_name=self.study_name,\n",
        "            storage=self.storage_name,\n",
        "            load_if_exists=True,\n",
        "            direction=\"maximize\"\n",
        "        )\n",
        "\n",
        "    def objective(self, trial, train_idx, val_idx):\n",
        "        # Hyperparameters to optimize\n",
        "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.7)\n",
        "        hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 256)\n",
        "        embedding_dim = trial.suggest_int(\"embedding_dim\", 16, 256)\n",
        "\n",
        "        # Build model\n",
        "        model = GCN(\n",
        "            num_nodes=self.data.num_nodes,\n",
        "            embedding_dim=embedding_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            out_dim=self.data.y.shape[1],\n",
        "            drop_out=dropout\n",
        "        )\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "        criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Subgraphs\n",
        "        train_subgraph = self.data.subgraph(torch.tensor(train_idx))\n",
        "        val_subgraph = self.data.subgraph(torch.tensor(val_idx))\n",
        "\n",
        "        # Train loop\n",
        "        epochs = 100\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            out = model(self.data.x, train_subgraph.edge_index)\n",
        "            loss = criterion(out, self.data.y.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(self.data.x, val_subgraph.edge_index)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).int()\n",
        "\n",
        "            y_true = self.data.y[val_idx].cpu().numpy()\n",
        "            y_pred = preds[val_idx].cpu().numpy()\n",
        "            y_prob = probs[val_idx].cpu().numpy()\n",
        "\n",
        "            auroc = roc_auc_score(y_true, y_prob, average='macro')\n",
        "\n",
        "        return auroc\n",
        "\n",
        "    def run_nested_cv(self, n_splits_outer=5, n_splits_inner=3, n_trials=20):\n",
        "        nodes_idx = self.data.edge_index.flatten().unique()\n",
        "        y = self.data.node_species.squeeze()\n",
        "\n",
        "        outer_skf = StratifiedKFold(n_splits=n_splits_outer, shuffle=True, random_state=42)\n",
        "\n",
        "        outer_results = []\n",
        "\n",
        "        for fold, (trainval_idx, test_idx) in enumerate(outer_skf.split(nodes_idx, y)):\n",
        "            print(f\"===== Outer Fold {fold+1}/{n_splits_outer} =====\")\n",
        "\n",
        "            # Inner CV for hyperparameter tuning\n",
        "            def optuna_objective(trial):\n",
        "                inner_skf = StratifiedKFold(n_splits=n_splits_inner, shuffle=True, random_state=42)\n",
        "                inner_scores = []\n",
        "\n",
        "                for inner_train_idx, inner_val_idx in inner_skf.split(trainval_idx, y[trainval_idx]):\n",
        "                    score = self.objective(trial, trainval_idx[inner_train_idx], trainval_idx[inner_val_idx])\n",
        "                    inner_scores.append(score)\n",
        "\n",
        "                return np.mean(inner_scores)\n",
        "\n",
        "            study = optuna.create_study(direction=\"maximize\")\n",
        "            study.optimize(optuna_objective, n_trials=n_trials)\n",
        "\n",
        "            best_params = study.best_params\n",
        "            print(f\"Best params for fold {fold+1}: {best_params}\")\n",
        "\n",
        "            # Retrain with best params on full trainval\n",
        "            model = GCN(\n",
        "                num_nodes=self.data.num_nodes,\n",
        "                embedding_dim=best_params[\"embedding_dim\"],\n",
        "                hidden_dim=best_params[\"hidden_dim\"],\n",
        "                out_dim=self.data.y.shape[1],\n",
        "                drop_out=best_params[\"dropout\"]\n",
        "            )\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "            criterion = torch.nn.BCEWithLogitsLoss()\n",
        "            trainval_subgraph = self.data.subgraph(torch.tensor(trainval_idx))\n",
        "            test_subgraph = self.data.subgraph(torch.tensor(test_idx))\n",
        "\n",
        "            for epoch in range(100):\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                out = model(self.data.x, trainval_subgraph.edge_index)\n",
        "                loss = criterion(out, self.data.y.float())\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Test evaluation\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                logits = model(self.data.x, test_subgraph.edge_index)\n",
        "                probs = torch.sigmoid(logits)\n",
        "                preds = (probs > 0.5).int()\n",
        "\n",
        "                y_true = self.data.y[test_idx].cpu().numpy()\n",
        "                y_pred = preds[test_idx].cpu().numpy()\n",
        "                y_prob = probs[test_idx].cpu().numpy()\n",
        "\n",
        "                acc = accuracy_score(y_true, y_pred)\n",
        "                auroc = roc_auc_score(y_true, y_prob, average='macro')\n",
        "                precision = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "                recall = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "                f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
        "\n",
        "                metrics = {\n",
        "                    \"accuracy\": acc,\n",
        "                    \"auroc\": auroc,\n",
        "                    \"precision\": precision,\n",
        "                    \"recall\": recall,\n",
        "                    \"f1\": f1,\n",
        "                    \"fold\": fold+1\n",
        "                }\n",
        "\n",
        "                outer_results.append(metrics)\n",
        "\n",
        "                # Log to wandb\n",
        "                wandb.log(metrics)\n",
        "\n",
        "        return outer_results\n"
      ],
      "metadata": {
        "id": "H7rX1UIc2MYo"
      },
      "id": "H7rX1UIc2MYo",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage:\n",
        "# wandb.init(project=\"gcn-nested-cv\")\n",
        "# optimizer = GCNOptimization(data)\n",
        "# results = optimizer.run_nested_cv(n_splits_outer=5, n_splits_inner=3, n_trials=20)\n",
        "# wandb.finish()"
      ],
      "metadata": {
        "id": "Kw1ab_SjP_2o"
      },
      "id": "Kw1ab_SjP_2o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bdd638c2",
      "metadata": {
        "id": "bdd638c2"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}